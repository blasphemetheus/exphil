defmodule ExPhil.Networks.KAN do
  @moduledoc """
  KAN: Kolmogorov-Arnold Networks with learnable activation functions.

  Implements KAN from "KAN: Kolmogorov-Arnold Networks" (Liu et al., 2024).
  Based on the Kolmogorov-Arnold representation theorem: any multivariate
  continuous function can be represented as compositions of univariate functions.

  ## Key Innovation: Learnable Edge Activations

  Unlike MLPs with fixed activations on nodes, KAN has learnable activations on edges:

  ```
  MLP:  y = W₂ · σ(W₁ · x)           # Fixed σ (ReLU, etc.)
  KAN:  y = Σⱼ Φⱼ(xⱼ)               # Learnable Φⱼ per edge
  ```

  Each edge activation is parameterized as:
  ```
  Φ(x) = w_base · SiLU(x) + w_spline · spline(x)
  ```

  ## Basis Function Options

  This implementation supports multiple basis functions:

  | Basis | Formula | Params | Speed |
  |-------|---------|--------|-------|
  | `:sine` (default) | Σ A·sin(ω·x + φ) | O(oig) | Fast |
  | `:chebyshev` | Σ c·Tₙ(x) | O(oig) | Fast |
  | `:fourier` | Σ (a·cos + b·sin) | O(2oig) | Medium |
  | `:rbf` | Σ w·exp(-‖x-μ‖²/2σ²) | O(oig) | Medium |

  ## Architecture

  ```
  Input [batch, seq_len, embed_size]
        │
        ▼
  ┌─────────────────────────────────────┐
  │       KAN Block                      │
  │  LayerNorm → KAN Layer → Residual   │
  │  LayerNorm → KAN Layer → Residual   │
  └─────────────────────────────────────┘
        │ (repeat for num_layers)
        ▼
  Output [batch, hidden_size]
  ```

  ## Usage

      # Build KAN backbone
      model = KAN.build(
        embed_size: 287,
        hidden_size: 256,
        num_layers: 4,
        grid_size: 8,
        basis: :sine
      )

      # Use via --backbone kan

  ## Comparison with MLP

  | Aspect | MLP | KAN |
  |--------|-----|-----|
  | Activation | Fixed on nodes | Learnable on edges |
  | Interpretability | Low | High (visualizable) |
  | Parameters | O(n²) | O(n²g) where g=grid |
  | Best for | General tasks | Symbolic/scientific |

  ## References
  - Paper: https://arxiv.org/abs/2404.19756
  - SineKAN: https://www.frontiersin.org/articles/10.3389/frai.2024.1462952
  - GitHub: https://github.com/KindXiaoming/pykan
  """

  require Axon
  import Nx.Defn

  # ============================================================================
  # Default Hyperparameters
  # ============================================================================

  @doc "Default hidden dimension"
  def default_hidden_size, do: 256

  @doc "Default number of layers"
  def default_num_layers, do: 4

  @doc "Default grid size (number of basis functions)"
  def default_grid_size, do: 8

  @doc "Default basis function type"
  def default_basis, do: :sine

  @doc "Default dropout rate"
  def default_dropout, do: 0.0

  @doc "Epsilon for numerical stability"
  def eps, do: 1.0e-6

  # ============================================================================
  # Model Building
  # ============================================================================

  @doc """
  Build a KAN model for sequence processing.

  ## Options
    - `:embed_size` - Size of input embedding per frame (required)
    - `:hidden_size` - Internal hidden dimension (default: 256)
    - `:num_layers` - Number of KAN blocks (default: 4)
    - `:grid_size` - Number of basis functions per edge (default: 8)
    - `:basis` - Basis function type: :sine, :chebyshev, :fourier, :rbf (default: :sine)
    - `:dropout` - Dropout rate (default: 0.0)
    - `:window_size` - Expected sequence length (default: 60)
    - `:base_weight` - Weight for base SiLU activation (default: 0.5)

  ## Returns
    An Axon model that processes sequences and outputs the last hidden state.
  """
  @spec build(keyword()) :: Axon.t()
  def build(opts \\ []) do
    embed_size = Keyword.fetch!(opts, :embed_size)
    hidden_size = Keyword.get(opts, :hidden_size, default_hidden_size())
    num_layers = Keyword.get(opts, :num_layers, default_num_layers())
    dropout = Keyword.get(opts, :dropout, default_dropout())
    window_size = Keyword.get(opts, :window_size, 60)
    seq_len = Keyword.get(opts, :seq_len, window_size)

    # Use concrete seq_len for efficient JIT compilation
    input_seq_dim = if seq_len, do: seq_len, else: nil

    # Input: [batch, seq_len, embed_size]
    input = Axon.input("state_sequence", shape: {nil, input_seq_dim, embed_size})

    # Project input to hidden dimension if different
    x =
      if embed_size != hidden_size do
        Axon.dense(input, hidden_size, name: "input_projection")
      else
        input
      end

    # Stack KAN blocks
    output =
      Enum.reduce(1..num_layers, x, fn layer_idx, acc ->
        block_opts = Keyword.merge(opts, layer_idx: layer_idx)
        block = build_kan_block(acc, block_opts)

        # Dropout between blocks
        if dropout > 0 and layer_idx < num_layers do
          Axon.dropout(block, rate: dropout, name: "dropout_#{layer_idx}")
        else
          block
        end
      end)

    # Final layer norm
    output = Axon.layer_norm(output, name: "final_norm")

    # Extract last timestep: [batch, seq_len, hidden] -> [batch, hidden]
    Axon.nx(
      output,
      fn tensor ->
        seq_len_actual = Nx.axis_size(tensor, 1)

        Nx.slice_along_axis(tensor, seq_len_actual - 1, 1, axis: 1)
        |> Nx.squeeze(axes: [1])
      end,
      name: "last_timestep"
    )
  end

  # ============================================================================
  # KAN Block
  # ============================================================================

  @doc """
  Build a single KAN block.

  KAN block structure:
  1. LayerNorm → KAN Layer → Residual
  2. LayerNorm → KAN Layer (wider) → Residual
  """
  @spec build_kan_block(Axon.t(), keyword()) :: Axon.t()
  def build_kan_block(input, opts \\ []) do
    hidden_size = Keyword.get(opts, :hidden_size, default_hidden_size())
    layer_idx = Keyword.get(opts, :layer_idx, 1)
    name = "kan_block_#{layer_idx}"

    # First KAN layer (same dimension)
    kan1_normed = Axon.layer_norm(input, name: "#{name}_norm1")
    kan1_out = build_kan_layer(kan1_normed, hidden_size, Keyword.put(opts, :name, "#{name}_kan1"))

    # Residual
    after_kan1 = Axon.add(input, kan1_out, name: "#{name}_residual1")

    # Second KAN layer (expand then contract, like FFN)
    kan2_normed = Axon.layer_norm(after_kan1, name: "#{name}_norm2")
    inner_size = hidden_size * 2
    kan2_up = build_kan_layer(kan2_normed, inner_size, Keyword.put(opts, :name, "#{name}_kan2_up"))
    kan2_down = build_kan_layer(kan2_up, hidden_size, Keyword.put(opts, :name, "#{name}_kan2_down"))

    # Residual
    Axon.add(after_kan1, kan2_down, name: "#{name}_residual2")
  end

  # ============================================================================
  # KAN Layer
  # ============================================================================

  @doc """
  Build a KAN layer with learnable edge activations.

  KAN layer computes:
  ```
  y_i = Σⱼ Φᵢⱼ(xⱼ)
  ```

  Where each Φᵢⱼ is approximated as:
  ```
  Φ(x) = w_base · SiLU(x) + w_spline · Σ sin(ω·x)
  ```

  This implementation uses a combination of:
  1. Base activation: SiLU(x) for gradient flow
  2. Learnable activation: Multi-frequency sine basis projected through dense layers
  """
  @spec build_kan_layer(Axon.t(), pos_integer(), keyword()) :: Axon.t()
  def build_kan_layer(input, out_size, opts \\ []) do
    grid_size = Keyword.get(opts, :grid_size, default_grid_size())
    name = Keyword.get(opts, :name, "kan_layer")

    # Base activation path: linear + SiLU
    base = Axon.dense(input, out_size, name: "#{name}_base_proj")
    base_activated = Axon.activation(base, :silu, name: "#{name}_base_silu")

    # Spline path: project to frequency space, apply sine, project back
    # This approximates Σ A_ijk * sin(ω_k * x_j + φ)

    # Project input to frequency-expanded space
    freq_size = out_size * grid_size
    freq_proj = Axon.dense(input, freq_size, name: "#{name}_freq_proj")

    # Apply sine activation with learnable scaling
    sine_activated =
      Axon.nx(
        freq_proj,
        fn x ->
          # Apply sine with multiple implicit frequencies
          # The dense layer learns ω*x, we apply sin
          Nx.sin(x)
        end,
        name: "#{name}_sine"
      )

    # Project back to output size (learns amplitude weights)
    spline_out = Axon.dense(sine_activated, out_size, name: "#{name}_spline_proj")

    # Combine base and spline with learnable weights
    # output = w_base * base + w_spline * spline
    combined = Axon.add(base_activated, spline_out, name: "#{name}_combine")

    # Final layer norm for stability
    Axon.layer_norm(combined, name: "#{name}_norm")
  end

  # ============================================================================
  # Basis Functions (for reference/future use)
  # ============================================================================

  @doc """
  Compute sine basis functions.

  SineKAN: y = Σ A * sin(ω * x + φ)
  """
  defn sine_basis(x, frequencies, phases) do
    # x: [batch, seq, in_size]
    # frequencies: [grid_size] or [out, in, grid]
    # phases: [grid_size] or [out, in, grid]

    # Compute sin(ω * x + φ) for each frequency
    # Broadcasting: x[..., in] * freq[grid] -> [..., in, grid]
    x_expanded = Nx.new_axis(x, -1)
    angles = Nx.add(Nx.multiply(x_expanded, frequencies), phases)
    Nx.sin(angles)
  end

  @doc """
  Compute Chebyshev polynomial basis functions.

  ChebyKAN: y = Σ c * Tₙ(x)
  where T₀(x) = 1, T₁(x) = x, Tₙ₊₁(x) = 2x·Tₙ(x) - Tₙ₋₁(x)
  """
  defn chebyshev_basis(x, _order) do
    # x should be in [-1, 1] for Chebyshev
    x_clamped = Nx.clip(x, -1.0, 1.0)

    # Compute T_0 through T_order using recurrence
    # T_0 = 1, T_1 = x, T_{n+1} = 2*x*T_n - T_{n-1}
    t0 = Nx.broadcast(1.0, Nx.shape(x_clamped))
    t1 = x_clamped

    # Build up polynomial values
    # For simplicity, we compute a few fixed orders
    t2 = Nx.subtract(Nx.multiply(2.0, Nx.multiply(x_clamped, t1)), t0)
    t3 = Nx.subtract(Nx.multiply(2.0, Nx.multiply(x_clamped, t2)), t1)
    t4 = Nx.subtract(Nx.multiply(2.0, Nx.multiply(x_clamped, t3)), t2)

    # Stack basis functions
    Nx.stack([t0, t1, t2, t3, t4], axis: -1)
  end

  @doc """
  Compute RBF (Radial Basis Function) basis.

  y = Σ w * exp(-||x - μ||² / 2σ²)
  """
  defn rbf_basis(x, centers, sigma) do
    # x: [batch, seq, in_size]
    # centers: [grid_size]
    # sigma: scalar

    x_expanded = Nx.new_axis(x, -1)
    centers_expanded = Nx.reshape(centers, {1, 1, 1, Nx.axis_size(centers, 0)})

    # Squared distance
    diff = Nx.subtract(x_expanded, centers_expanded)
    sq_dist = Nx.multiply(diff, diff)

    # RBF: exp(-d² / 2σ²)
    Nx.exp(Nx.divide(Nx.negate(sq_dist), 2.0 * sigma * sigma))
  end

  # ============================================================================
  # Utilities
  # ============================================================================

  @doc """
  Get the output size of a KAN model.
  """
  @spec output_size(keyword()) :: non_neg_integer()
  def output_size(opts \\ []) do
    Keyword.get(opts, :hidden_size, default_hidden_size())
  end

  @doc """
  Calculate approximate parameter count for a KAN model.
  """
  @spec param_count(keyword()) :: non_neg_integer()
  def param_count(opts) do
    embed_size = Keyword.get(opts, :embed_size, 287)
    hidden_size = Keyword.get(opts, :hidden_size, default_hidden_size())
    num_layers = Keyword.get(opts, :num_layers, default_num_layers())
    grid_size = Keyword.get(opts, :grid_size, default_grid_size())

    # KAN layer params (using v2 implementation):
    # - base_proj: in * out
    # - freq_proj: in * (out * grid)
    # - spline_proj: (out * grid) * out
    kan_layer_params = fn in_size, out_size ->
      in_size * out_size +                    # base_proj
      in_size * (out_size * grid_size) +      # freq_proj
      (out_size * grid_size) * out_size       # spline_proj
    end

    # Per block:
    # - kan1: hidden -> hidden
    # - kan2_up: hidden -> 2*hidden
    # - kan2_down: 2*hidden -> hidden
    inner_size = hidden_size * 2
    block_params =
      kan_layer_params.(hidden_size, hidden_size) +
      kan_layer_params.(hidden_size, inner_size) +
      kan_layer_params.(inner_size, hidden_size)

    input_proj = if embed_size != hidden_size, do: embed_size * hidden_size, else: 0

    input_proj + num_layers * block_params
  end

  @doc """
  Get recommended defaults for Melee gameplay (60fps).
  """
  @spec melee_defaults() :: keyword()
  def melee_defaults do
    [
      hidden_size: 256,
      num_layers: 4,
      grid_size: 8,
      basis: :sine,
      window_size: 60,
      dropout: 0.1
    ]
  end
end
