# S5 Architecture

**Type:** Simplified State Space Sequence Model
**Complexity:** O(n) training and inference
**60 FPS Ready:** Yes (~9ms inference)

## Overview

S5 (Simplified State Space Sequence model) is a streamlined version of structured state space models (S4). It removes the complex diagonal-plus-low-rank (DPLR) parameterization of S4 in favor of simple diagonal state matrices, achieving comparable performance with much simpler implementation.

## Etymology

**S5** stands for **S**implified **S**tructured **S**tate **S**pace **S**equence model. The "5" comes from adding one more "S" to "S4" (which was Structured State Space Sequence model). It's a play on words indicating "S4 + 1 = S5" where the "+1" represents the simplification.

## Architecture

```
State Space Model (continuous-time):
dx/dt = A·x + B·u     (state evolution)
y = C·x + D·u         (output)

Discretized for sequences:
x_t = Ā·x_{t-1} + B̄·u_t
y_t = C·x_t + D·u_t

S5 Simplification:
A = diag(λ_1, λ_2, ..., λ_n)  ← Diagonal! (S4 uses DPLR)
```

The key insight: Diagonal A matrices are sufficient for most tasks and enable much simpler parallel computation.

## When to Use

**Choose S5 when:**
- You want a simple, well-understood SSM
- Mathematical elegance matters (clean state space formulation)
- You're implementing from scratch (easiest SSM to code)

**Avoid S5 when:**
- You need maximum performance (Mamba is usually better)
- Very long sequences (> 4000 steps) where S4's DPLR helps

## Configuration

```bash
# Basic usage
mix run scripts/train_from_replays.exs --temporal --backbone s5

# With custom settings
mix run scripts/train_from_replays.exs \
  --temporal \
  --backbone s5 \
  --hidden-size 256 \
  --state-size 64 \
  --num-layers 6
```

### Options

| Option | Default | Description |
|--------|---------|-------------|
| `hidden_size` | 256 | Model dimension |
| `state_size` | 64 | SSM state dimension |
| `num_layers` | 6 | Number of S5 blocks |
| `dt_min` | 0.001 | Minimum discretization step |
| `dt_max` | 0.1 | Maximum discretization step |

## Implementation

```elixir
# lib/exphil/networks/s5.ex
defmodule ExPhil.Networks.S5 do
  @moduledoc """
  S5: Simplified State Space Sequence model.
  Uses diagonal state matrices for efficiency.
  """

  def build(input, opts \\ []) do
    hidden_size = opts[:hidden_size] || 256
    state_size = opts[:state_size] || 64
    num_layers = opts[:num_layers] || 6

    Enum.reduce(1..num_layers, input, fn _layer, x ->
      x
      |> s5_block(hidden_size, state_size, opts)
      |> layer_norm()
    end)
  end

  defp s5_block(x, hidden_size, state_size, opts) do
    # Initialize diagonal A matrix (complex eigenvalues)
    # A = diag(λ_1, ..., λ_n) where λ_i are learned

    # Discretize: Ā = exp(A·Δt)
    # For diagonal A: Ā = diag(exp(λ_1·Δt), ...)

    # B, C, D are learned projections
    b = Axon.dense(x, state_size)
    c_proj = Axon.dense(x, state_size)

    # Parallel scan for efficient training
    # x_t = Ā·x_{t-1} + B̄·u_t
    state_sequence = parallel_scan(a_bar, b)

    # Output: y_t = C·x_t
    Nx.dot(state_sequence, c_proj)
  end
end
```

## Performance

| Metric | Value | Notes |
|--------|-------|-------|
| Training | O(n) | Parallel scan |
| Inference | O(1) memory | Recurrent mode |
| Simplicity | High | Easiest SSM to implement |

### Benchmark (RTX 4090)

| Sequence Length | Training | Inference |
|-----------------|----------|-----------|
| 30 frames | 10ms | 9ms |
| 60 frames | 18ms | 9ms |
| 120 frames | 35ms | 9ms |

## S5 vs S4: The Simplification

**S4 (original):**
```
A = V · (Λ + P·P^T) · V^{-1}   # Diagonal + low-rank
```
- Complex parameterization (DPLR)
- HiPPO initialization for long-range
- Harder to implement correctly

**S5 (simplified):**
```
A = diag(λ_1, λ_2, ..., λ_n)   # Just diagonal
```
- Simple diagonal matrix
- Learned or random initialization
- Easy to implement

The surprising result: This simplification loses little performance on most tasks!

## The Parallel Scan

S5 (like all SSMs) uses the parallel scan algorithm for O(n log n) training:

```
Sequential (O(n)):
x_1 = Ā·x_0 + B̄·u_1
x_2 = Ā·x_1 + B̄·u_2
x_3 = Ā·x_2 + B̄·u_3
...

Parallel scan (O(log n) depth):
Combine pairs: (Ā, B̄·u) ⊗ (Ā, B̄·u) = (Ā², Ā·B̄·u + B̄·u)
Recursively until done
```

This is the key to efficient SSM training - the recurrence can be parallelized!

## Mathematical Foundation

The state space model comes from control theory:

1. **State** x: Internal memory of the system
2. **Input** u: Current observation
3. **Output** y: Predicted next action

The matrices (A, B, C, D) define:
- A: How state evolves over time (dynamics)
- B: How input affects state (control)
- C: How state maps to output (observation)
- D: Direct input-to-output (skip connection)

## Comparison with Other SSMs

| Model | A Matrix | Complexity | Performance |
|-------|----------|------------|-------------|
| S4 | DPLR | O(n log n) | Excellent |
| **S5** | Diagonal | O(n log n) | Very good |
| Mamba | Input-dependent | O(n) | Excellent |
| DSS | Diagonal | O(n log n) | Good |

## References

- [Simplified State Space Layers for Sequence Modeling](https://arxiv.org/abs/2208.04933) - S5 paper
- [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396) - S4 paper
- [The Annotated S4](https://srush.github.io/annotated-s4/) - Excellent tutorial
- [ExPhil Implementation](../../../lib/exphil/networks/s5.ex)

## See Also

- [MAMBA.md](MAMBA.md) - Input-selective SSM
- [MAMBA2_SSD.md](MAMBA2_SSD.md) - SSM-attention duality
- [GATED_SSM.md](GATED_SSM.md) - Gated variant
- [ARCHITECTURE_GUIDE.md](ARCHITECTURE_GUIDE.md) - All architectures overview
